# Editorial Workers

This directory contains the background workers responsible for the automated news processing pipeline of Tacitus.news. These workers operate asynchronously, processing items from various database queues to transform raw web content into structured, analyzed news events.

## ðŸ”„ Pipeline Overview

The editorial system operates as a directed acyclic graph (mostly) of queues. Data flows through the following stages:

1.  **Ingestion**: The **Harvester** fetches raw articles from RSS feeds and websites.
2.  **Article Processing**:
    *   **Filter**: Checks article relevance.
    *   **Enricher**: Scrapes full content and metadata.
    *   **Analyzer**: Uses LLMs to analyze stance, bias, and entities.
3.  **Organization**: The **Cluster** worker groups analyzed articles into **Events**.
4.  **Event Lifecycle**:
    *   **Enhancer**: Generates event titles and summaries.
    *   **Publisher**: Scores events and manages their visibility (Draft -> Published).
5.  **Refinement & Maintenance**:
    *   **Merger** & **Reviewer**: Continuously scan for and resolve duplicate events/articles.
    *   **Gardener**: Performs system hygiene, archival, and score decay.

---

## ðŸ‘· Worker Descriptions

### 1. Harvester (`harvester.py`)
*   **Role**: The entry point of the system. Periodically scans configured `Newspapers` and `Feeds`.
*   **Input**: Schedule / configuration.
*   **Output**: Creates `ArticleModel` records and pushes them to the **Filter** or **Enricher** queue.
*   **Key Logic**: Handles RSS parsing, duplication checks (URL hash), and concurrent fetching.

### 2. Filter (`filter.py`)
*   **Role**: A "gatekeeper" that cheaply filters out irrelevant content before expensive processing.
*   **Input**: `ArticlesQueueName.FILTER`
*   **Output**: Routes accepted articles to **Enricher**; archives rejected ones.
*   **Key Logic**: Batched LLM calls to validate titles against editorial guidelines.

### 3. Enricher (`enricher.py`)
*   **Role**: Ensures articles have full content (scraping) and necessary metadata.
*   **Input**: `ArticlesQueueName.ENRICHER`
*   **Output**: Routes to **Analyzer** (or back to Filter if titles were missing).
*   **Key Logic**: Uses `ContentEnricherDomain` to scrape bodies and extract authors/dates.

### 4. Analyzer (`analyzer.py`)
*   **Role**: The core intelligence worker. Understands the content of the article.
*   **Input**: `ArticlesQueueName.ANALYZER`
*   **Output**: Routes to **Cluster**.
*   **Key Logic**: Uses LLMs to generate summaries, extract entities, determine political stance/bias, and detect clickbait.

### 5. Cluster (`cluster.py`)
*   **Role**: Groups individual articles into stories (Events).
*   **Input**: `ArticlesQueueName.CLUSTER`
*   **Output**: Creates new `NewsEventModel` records (queues to **Enhancer**) or generates `MergeProposals` (queues to **Reviewer**).
*   **Key Logic**: Vector similarity and time-window clustering to decide if an article belongs to an existing event or starts a new one.

### 6. Enhancer (`enhancer.py`)
*   **Role**: polishes raw events into readable news items.
*   **Input**: `EventsQueueName.ENHANCER`
*   **Output**: Routes to **Publisher**.
*   **Key Logic**: Generates the "Golden Title" and "Golden Summary" for an event based on its constituent articles.

### 7. Publisher (`publisher.py`)
*   **Role**: The final gatekeeper for events.
*   **Input**: `EventsQueueName.PUBLISHER`
*   **Output**: Updates `NewsEventModel` status to `PUBLISHED`.
*   **Key Logic**: Calculates `Hot Score`, evaluates "Blind Spots", and determines if an event meets the quality threshold for the front page.

### 8. Reviewer (`reviewer.py`)
*   **Role**: Resolves merge proposals generated by the Cluster or Merger workers.
*   **Input**: `MergeProposalModel` table.
*   **Output**: Executes merges (moving articles between events) or rejects proposals.
*   **Key Logic**: LLM-based decision making to compare source vs. target semantic similarity.

### 9. Merger (`merger.py`)
*   **Role**: A proactive scanner that looks for duplicate events that the Cluster worker might have missed.
*   **Input**: Active `NewsEventModel` records.
*   **Output**: Creates `MergeProposalModel` records for the **Reviewer**.
*   **Key Logic**: Producer/Consumer pattern scanning recent events for high similarity.

### 10. Gardener (`gardener.py`)
*   **Role**: System maintenance and janitorial tasks.
*   **Input**: Time-based schedule.
*   **Output**: Database updates (Archiving, Score Decay).
*   **Key Logic**:
    *   **Janitor**: Archives old data.
    *   **Score Decay**: Gradually lowers the "Hot Score" of events over time so new news can rise.
    *   **Splitter**: (Optional) detecting over-clustered events.

---

## ðŸš€ Running Workers

Workers are typically executed as standalone processes or via the main CLI entry point.

Example (running a specific worker):
```bash
python services/editorial/app/workers/analyzer.py
```

They are designed to be resilient, handling database reconnections and trapping signals for graceful shutdowns.
